{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Reads data from downloaded the [UCSD Book Graph](https://sites.google.com/eng.ucsd.edu/ucsdbookgraph/home), parses it into smaller csv files, and combines the csv files into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv, glob, gzip, os, math, joblib\n",
    "import pandas as pd\n",
    "\n",
    "# directory where data is stored\n",
    "load_DIR = '/media/mthommes/MThommes/Insight/'\n",
    "# directory where chunks will be stored\n",
    "chunks_DIR = '/home/mthommes/Documents/insight/reviews/'\n",
    "# directory where data will be saved\n",
    "save_DIR = '/home/mthommes/Documents/GitHub/insight/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "**count_data:** Count the number of lines in a json.gz file\n",
    "\n",
    "**make_map:** Create a map between \\*\\_id\\_csv and \\*\\_id, where \\* can be book or user.\n",
    "\n",
    "**parse_data:** Parse a json.gz file into smaller csv files\n",
    "\n",
    "**csv_to_df:** Convert all csv files in a folder into a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_data(file_name):\n",
    "    \"\"\"\n",
    "    count_data counts the number of lines in a json.gz file\n",
    "    INPUT\n",
    "    file_name: full path of file to read (str)\n",
    "    OUTPUT\n",
    "    count: number of lines (int)\n",
    "    \"\"\"\n",
    "    with gzip.open(file_name) as f_open:\n",
    "        count = 0\n",
    "        for line in f_open:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def make_map(file_name, dtype):\n",
    "    \"\"\"\n",
    "    make_map creates a map between *_id_csv and *_id\n",
    "    INPUTS\n",
    "    file_name: full path of file to read (str)\n",
    "    dtype: data type of *id_csv and *id (dict)\n",
    "    OUTPUT\n",
    "    mapping: (dict)\n",
    "    \"\"\"\n",
    "    map_df = pd.read_csv(file_name, dtype=dtype,\n",
    "                         skipinitialspace=True)\n",
    "    mapping = dict([(v,k) for k,v in map_df.values]) # create mapping between *_id_csv and *_id\n",
    "    return mapping\n",
    "\n",
    "def parse_data(file_name, chunks_DIR, user_map, book_map, chunkSize=100):\n",
    "    \"\"\"\n",
    "    parse_data into csv files with lines of chunkSize\n",
    "    INPUTS\n",
    "    file_name: full path of file to read (str)\n",
    "    chunks_DIR: full path to save chunks (str)\n",
    "    user_map: map to rename user_ids (dict)\n",
    "    book_map: map to rename book_ids (dict)\n",
    "    chunkSize: number of lines to read (int)\n",
    "    \"\"\"\n",
    "    chunks = pd.read_json(file_name, lines=True, chunksize=chunkSize)\n",
    "    for i, c in enumerate(chunks):\n",
    "        print('...chunk',i)\n",
    "        # change user_ids from str to int\n",
    "        c['user_id'] = c['user_id'].map(user_map)\n",
    "        c['book_id'] = c['book_id'].map(book_map)\n",
    "        c.to_csv(os.path.join(chunks_DIR, 'chunk_{}.csv').format(i))\n",
    "\n",
    "def csv_to_df(chunks_DIR, col_names, n_files=1):\n",
    "    \"\"\"\n",
    "    csv_to_df converts csv files to a single DataFrame\n",
    "    INPUTS\n",
    "    chunks_DIR: full path to chunk files (str)\n",
    "    col_names: column names to save (list)\n",
    "    n_files: number of chunk files\n",
    "    OUTPUT\n",
    "    df: data (DataFrame)\n",
    "    \"\"\"\n",
    "    # grab all files from the directory\n",
    "    files = glob.glob(os.path.join(chunks_DIR, 'chunk_*.csv'))\n",
    "    if len(files) != n_files:\n",
    "        print('Error:', len(files), 'out of', n_files, 'found')\n",
    "    else:\n",
    "        print('All files found')\n",
    "    # concatenate files into df and add chunk number as a new column to the df\n",
    "    df = pd.concat([pd.read_csv(f, usecols=col_names).assign(New=os.path.basename(f).split('.')[0].split('_')[1]) for f in files])\n",
    "    # rename the new column\n",
    "    df.rename(columns={'New':'Chunk'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maps\n",
    "\n",
    "Need to convert user_ids and book_ids to numbers instead of strings (because they are smaller to store in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping between user_id_csv and user_id\n",
    "file_name = 'user_id_map.csv'\n",
    "user_map = make_map(os.path.join(load_DIR, file_name),\n",
    "                    {'user_id_csv':int, 'user_id':str})\n",
    "\n",
    "# create mapping between book_id_csv and book_id\n",
    "file_name = 'book_id_map.csv'\n",
    "book_map = make_map(os.path.join(load_DIR, file_name),\n",
    "                    {'book_id_csv':int, 'book_id':int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Data Points\n",
    "\n",
    "There are ~15.7M reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15739967 lines\n"
     ]
    }
   ],
   "source": [
    "load_name = 'goodreads_reviews_dedup.json.gz'\n",
    "\n",
    "n_lines = count_data(os.path.join(load_DIR, load_name))\n",
    "print('There are', n_lines, 'lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Data into CSV\n",
    "\n",
    "Because the files are very large (millions of lines of json strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing into 158 files...\n",
      "...chunk 0\n",
      "...chunk 1\n",
      "...chunk 2\n",
      "...chunk 3\n",
      "...chunk 4\n",
      "...chunk 5\n",
      "...chunk 6\n",
      "...chunk 7\n",
      "...chunk 8\n",
      "...chunk 9\n",
      "...chunk 10\n",
      "...chunk 11\n",
      "...chunk 12\n",
      "...chunk 13\n",
      "...chunk 14\n",
      "...chunk 15\n",
      "...chunk 16\n",
      "...chunk 17\n",
      "...chunk 18\n",
      "...chunk 19\n",
      "...chunk 20\n",
      "...chunk 21\n",
      "...chunk 22\n",
      "...chunk 23\n",
      "...chunk 24\n",
      "...chunk 25\n",
      "...chunk 26\n",
      "...chunk 27\n",
      "...chunk 28\n",
      "...chunk 29\n",
      "...chunk 30\n",
      "...chunk 31\n",
      "...chunk 32\n",
      "...chunk 33\n",
      "...chunk 34\n",
      "...chunk 35\n",
      "...chunk 36\n",
      "...chunk 37\n",
      "...chunk 38\n",
      "...chunk 39\n",
      "...chunk 40\n",
      "...chunk 41\n",
      "...chunk 42\n",
      "...chunk 43\n",
      "...chunk 44\n",
      "...chunk 45\n",
      "...chunk 46\n",
      "...chunk 47\n",
      "...chunk 48\n",
      "...chunk 49\n",
      "...chunk 50\n",
      "...chunk 51\n",
      "...chunk 52\n",
      "...chunk 53\n",
      "...chunk 54\n",
      "...chunk 55\n",
      "...chunk 56\n",
      "...chunk 57\n",
      "...chunk 58\n",
      "...chunk 59\n",
      "...chunk 60\n",
      "...chunk 61\n",
      "...chunk 62\n",
      "...chunk 63\n",
      "...chunk 64\n",
      "...chunk 65\n",
      "...chunk 66\n",
      "...chunk 67\n",
      "...chunk 68\n",
      "...chunk 69\n",
      "...chunk 70\n",
      "...chunk 71\n",
      "...chunk 72\n",
      "...chunk 73\n",
      "...chunk 74\n",
      "...chunk 75\n",
      "...chunk 76\n",
      "...chunk 77\n",
      "...chunk 78\n",
      "...chunk 79\n",
      "...chunk 80\n",
      "...chunk 81\n",
      "...chunk 82\n",
      "...chunk 83\n",
      "...chunk 84\n",
      "...chunk 85\n",
      "...chunk 86\n",
      "...chunk 87\n",
      "...chunk 88\n",
      "...chunk 89\n",
      "...chunk 90\n",
      "...chunk 91\n",
      "...chunk 92\n",
      "...chunk 93\n",
      "...chunk 94\n",
      "...chunk 95\n",
      "...chunk 96\n",
      "...chunk 97\n",
      "...chunk 98\n",
      "...chunk 99\n",
      "...chunk 100\n",
      "...chunk 101\n",
      "...chunk 102\n",
      "...chunk 103\n",
      "...chunk 104\n",
      "...chunk 105\n",
      "...chunk 106\n",
      "...chunk 107\n",
      "...chunk 108\n",
      "...chunk 109\n",
      "...chunk 110\n",
      "...chunk 111\n",
      "...chunk 112\n",
      "...chunk 113\n",
      "...chunk 114\n",
      "...chunk 115\n",
      "...chunk 116\n",
      "...chunk 117\n",
      "...chunk 118\n",
      "...chunk 119\n",
      "...chunk 120\n",
      "...chunk 121\n",
      "...chunk 122\n",
      "...chunk 123\n",
      "...chunk 124\n",
      "...chunk 125\n",
      "...chunk 126\n",
      "...chunk 127\n",
      "...chunk 128\n",
      "...chunk 129\n",
      "...chunk 130\n",
      "...chunk 131\n",
      "...chunk 132\n",
      "...chunk 133\n",
      "...chunk 134\n",
      "...chunk 135\n",
      "...chunk 136\n",
      "...chunk 137\n",
      "...chunk 138\n",
      "...chunk 139\n",
      "...chunk 140\n",
      "...chunk 141\n",
      "...chunk 142\n",
      "...chunk 143\n",
      "...chunk 144\n",
      "...chunk 145\n",
      "...chunk 146\n",
      "...chunk 147\n",
      "...chunk 148\n",
      "...chunk 149\n",
      "...chunk 150\n",
      "...chunk 151\n",
      "...chunk 152\n",
      "...chunk 153\n",
      "...chunk 154\n",
      "...chunk 155\n",
      "...chunk 156\n",
      "...chunk 157\n"
     ]
    }
   ],
   "source": [
    "chunkSize = 100000\n",
    "\n",
    "n_files = math.ceil(n_lines/chunkSize)\n",
    "\n",
    "print('Parsing into', n_files, 'files...')\n",
    "parse_data(os.path.join(load_DIR, load_name),\n",
    "           chunks_DIR, user_map, book_map, chunkSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Data to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data to dataframe...\n",
      "All files found\n"
     ]
    }
   ],
   "source": [
    "n_files = 158\n",
    "# aggregate csv files into single DataFrame\n",
    "col_names = ['user_id','book_id','rating','read_at','started_at']\n",
    "print('Converting data to dataframe...')\n",
    "df = csv_to_df(chunks_DIR, col_names, n_files=n_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "...saved!\n"
     ]
    }
   ],
   "source": [
    "save_name = 'reviews.gz'\n",
    "print('Saving data...')\n",
    "joblib.dump(df, os.path.join(save_DIR, save_name), compress=3);\n",
    "print('...saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
